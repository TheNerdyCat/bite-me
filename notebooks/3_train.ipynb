{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef0d2692",
   "metadata": {},
   "source": [
    "# BiteMe | Train\n",
    "\n",
    "This notebook includes the most important part of the project - the modelling. The notebook tests methodologies for training, and in it the chosen algorithm is decided. Validation also occurs before final testing, which is conducted in the test notebook. This stage is highly iterative, so all model artefacts, logs and configurations are recorded and saved to disk automatically. This initial setup of what will eventually become MLOps for the final product will be really useful, and helps keep track of what is successful and what isn't.\n",
    "\n",
    "Models to try:\n",
    " - resnet50v2\n",
    " - resnet101v2\n",
    " - resnet152v2\n",
    " - vgg19\n",
    " - densenet169\n",
    " - densenet121\n",
    " - densenet201\n",
    " - inceptionv3\n",
    " - inception_resnetv2\n",
    " - resnext50\n",
    " - resnext101\n",
    " - xception\n",
    " - efficientnet_b0\n",
    " - efficientnet_b1\n",
    " - efficientnet_b2\n",
    " - efficientnet_b3\n",
    " - efficientnet_b4\n",
    " - efficientnet_b5\n",
    "\n",
    "Initial model work is done by using simple, typical image recognition models (CNN architectures) to see how effective these models can be for the problem. Although I don't \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8b5859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn\n",
    "\n",
    "# Modelling imports\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "import torch\n",
    "\n",
    "# Local imports\n",
    "sys.path.append(\"..\")\n",
    "from helpers import read_images, augs, get_augs\n",
    "from constants import *\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 8)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "ia.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a79a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "base_dir_path = \"../\"\n",
    "\n",
    "data_dir_path = os.path.join(base_dir_path, \"data\")\n",
    "data_preprocessed_dir_path = os.path.join(data_dir_path, \"preprocessed\")\n",
    "data_preprocessed_train_dir_path = os.path.join(data_dir_path, \"preprocessed/train\")\n",
    "\n",
    "data_dir = os.listdir(data_dir_path)\n",
    "data_preprocessed_dir = os.listdir(data_preprocessed_dir_path)\n",
    "data_preprocessed_train_dir = os.listdir(data_preprocessed_train_dir_path)\n",
    "\n",
    "metadata_preprocessed_path = os.path.join(data_preprocessed_dir_path, \"metadata.csv\")\n",
    "metadata = pd.read_csv(metadata_preprocessed_path)\n",
    "# Subset to train only\n",
    "metadata = metadata.loc[metadata.split == \"train\"]\n",
    "\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dde582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in train images\n",
    "X_train = read_images(\n",
    "    data_dir_path=data_preprocessed_train_dir_path, \n",
    "    rows=ROWS, \n",
    "    cols=COLS, \n",
    "    channels=CHANNELS, \n",
    "    write_images=False, \n",
    "    output_data_dir_path=None,\n",
    "    verbose=VERBOSE\n",
    ")\n",
    "\n",
    "# Get labels\n",
    "y_train = np.array(metadata[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9eb1b2",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d4a78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose augmentations to use in preprocessing\n",
    "# For full list see helpers.py\n",
    "augs_to_select = [\n",
    "    \"Fliplr\", \n",
    "    \"Flipud\", \n",
    "    \"Cutout\"\n",
    "]\n",
    "# Subset augs based on those selected\n",
    "augs = dict((aug_name, augs[aug_name]) for aug_name in augs_to_select)\n",
    "\n",
    "# Modelling constants - add this to constants.py when needed\n",
    "MODEL_NAME = \"resnet50v2\"\n",
    "EPOCHS = 6\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionary of configurations used in modelling\n",
    "# this will be updated as modelling progresses if necessary, for logging\n",
    "conf = {\n",
    "    \"device\": \"cuda\" if torch.cuda.device_count() > 0 else \"cpu\",\n",
    "    \"device_name\": torch.cuda.get_device_name(0),\n",
    "    \"n_workers\": torch.cuda.device_count(),\n",
    "    \"rows\": ROWS,\n",
    "    \"cols\": COLS,\n",
    "    \"channels\": CHANNELS,\n",
    "    \"seed\": SEED,\n",
    "    \"num_classes\": list(metadata[\"label\"].unique()),\n",
    "    \"classes\": np.unique(y_train, return_counts=True)[0],\n",
    "    \"class_counts\": np.unique(y_train, return_counts=True)[1],\n",
    "    \"test_size\": TEST_SIZE,\n",
    "    \"num_train_sample\": y_train.shape[0],\n",
    "    \"val_size\": 0.15,\n",
    "    \"num_augs\": len(augs),\n",
    "    \"augs\": augs,\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"lr\": 1e-5,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"n_splits\": N_SPLITS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac202af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4cd7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a1b595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb53b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aebdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4d2995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split cross validation idx\n",
    "# Subset images and labels for cross validation\n",
    "# Create image augmentations and additional labels\n",
    "# Read in pretrained weights\n",
    "# Any additional layers\n",
    "# Create model instance\n",
    "# Create error metric\n",
    "# Run training\n",
    "# Make val predictions\n",
    "# Val error metric\n",
    "# Create directory for instance\n",
    "# Save model\n",
    "# Save log and config \n",
    "# Append train/val errors to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c68f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329e0f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2ed2cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8f7584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=3)\n",
    "for train_index, test_index in skf.split(metadata.index, metadata[\"label\"]):\n",
    "    print(train_index)\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13a6d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85aed5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64da8274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edwardsims/miniconda3/envs/biteme/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9f9f5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training using cuda...\n",
      "[INFO] preparing data...\n",
      "Sequential(\n",
      "  (hidden_layer_1): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (activation_1): ReLU()\n",
      "  (output_layer): Linear(in_features=8, out_features=3, bias=True)\n",
      ")\n",
      "[INFO] epoch: 1...\n",
      "epoch: 1 train loss: 0.024 train accuracy: 0.991\n",
      "[INFO] epoch: 2...\n",
      "epoch: 2 train loss: 0.021 train accuracy: 0.992\n",
      "[INFO] epoch: 3...\n",
      "epoch: 3 train loss: 0.021 train accuracy: 0.993\n",
      "[INFO] epoch: 4...\n",
      "epoch: 4 train loss: 0.021 train accuracy: 0.993\n",
      "[INFO] epoch: 5...\n",
      "epoch: 5 train loss: 0.021 train accuracy: 0.993\n",
      "[INFO] epoch: 6...\n",
      "epoch: 6 train loss: 0.020 train accuracy: 0.993\n",
      "[INFO] epoch: 7...\n",
      "epoch: 7 train loss: 0.020 train accuracy: 0.993\n",
      "[INFO] epoch: 8...\n",
      "epoch: 8 train loss: 0.020 train accuracy: 0.993\n",
      "[INFO] epoch: 9...\n",
      "epoch: 9 train loss: 0.020 train accuracy: 0.993\n",
      "[INFO] epoch: 10...\n",
      "epoch: 10 train loss: 0.020 train accuracy: 0.993\n",
      "CPU times: user 11min 5s, sys: 8.12 s, total: 11min 13s\n",
      "Wall time: 11min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_training_model(inFeatures=4, hiddenDim=8, nbClasses=3):\n",
    "    # construct a shallow, sequential neural network\n",
    "    mlpModel = nn.Sequential(OrderedDict([\n",
    "        (\"hidden_layer_1\", nn.Linear(inFeatures, hiddenDim)),\n",
    "        (\"activation_1\", nn.ReLU()),\n",
    "        (\"output_layer\", nn.Linear(hiddenDim, nbClasses))\n",
    "    ]))\n",
    "    # return the sequential model\n",
    "    return mlpModel\n",
    "\n",
    "def next_batch(inputs, targets, batchSize):\n",
    "    # loop over the dataset\n",
    "    for i in range(0, inputs.shape[0], batchSize):\n",
    "        # yield a tuple of the current batched data and labels\n",
    "        yield (inputs[i:i + batchSize], targets[i:i + batchSize])\n",
    "        \n",
    "# specify our batch size, number of epochs, and learning rate\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LR = 1e-2\n",
    "# determine the device we will be using for training\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"[INFO] training using {}...\".format(DEVICE))\n",
    "\n",
    "# generate a 3-class classification problem with 1000 data points,\n",
    "# where each data point is a 4D feature vector\n",
    "print(\"[INFO] preparing data...\")\n",
    "(X, y) = make_blobs(n_samples=10000000, n_features=4, centers=3,\n",
    "    cluster_std=2.5, random_state=95)\n",
    "# create training and testing splits, and convert them to PyTorch\n",
    "# tensors\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, y,\n",
    "    test_size=0.15, random_state=95)\n",
    "trainX = torch.from_numpy(trainX).float()\n",
    "testX = torch.from_numpy(testX).float()\n",
    "trainY = torch.from_numpy(trainY).float()\n",
    "testY = torch.from_numpy(testY).float()\n",
    "\n",
    "# initialize our model and display its architecture\n",
    "mlp = get_training_model().to(DEVICE)\n",
    "print(mlp)\n",
    "# initialize optimizer and loss function\n",
    "opt = SGD(mlp.parameters(), lr=LR)\n",
    "lossFunc = nn.CrossEntropyLoss()\n",
    "\n",
    "# create a template to summarize current training progress\n",
    "trainTemplate = \"epoch: {} test loss: {:.3f} test accuracy: {:.3f}\"\n",
    "# loop through the epochs\n",
    "for epoch in range(0, EPOCHS):\n",
    "    # initialize tracker variables and set our model to trainable\n",
    "    print(\"[INFO] epoch: {}...\".format(epoch + 1))\n",
    "    trainLoss = 0\n",
    "    trainAcc = 0\n",
    "    samples = 0\n",
    "    mlp.train()\n",
    "    # loop over the current batch of data\n",
    "    for (batchX, batchY) in next_batch(trainX, trainY, BATCH_SIZE):\n",
    "        # flash data to the current device, run it through our\n",
    "        # model, and calculate loss\n",
    "        (batchX, batchY) = (batchX.to(DEVICE), batchY.to(DEVICE))\n",
    "        predictions = mlp(batchX)\n",
    "        loss = lossFunc(predictions, batchY.long())\n",
    "        # zero the gradients accumulated from the previous steps,\n",
    "        # perform backpropagation, and update model parameters\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        # update training loss, accuracy, and the number of samples\n",
    "        # visited\n",
    "        trainLoss += loss.item() * batchY.size(0)\n",
    "        trainAcc += (predictions.max(1)[1] == batchY).sum().item()\n",
    "        samples += batchY.size(0)\n",
    "    # display model progress on the current training batch\n",
    "    trainTemplate = \"epoch: {} train loss: {:.3f} train accuracy: {:.3f}\"\n",
    "    print(trainTemplate.format(epoch + 1, (trainLoss / samples),\n",
    "        (trainAcc / samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98740b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a6e928b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training using cpu...\n",
      "[INFO] preparing data...\n",
      "Sequential(\n",
      "  (hidden_layer_1): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (activation_1): ReLU()\n",
      "  (output_layer): Linear(in_features=8, out_features=3, bias=True)\n",
      ")\n",
      "[INFO] epoch: 1...\n",
      "epoch: 1 train loss: 0.024 train accuracy: 0.991\n",
      "[INFO] epoch: 2...\n",
      "epoch: 2 train loss: 0.021 train accuracy: 0.992\n",
      "[INFO] epoch: 3...\n",
      "epoch: 3 train loss: 0.021 train accuracy: 0.993\n",
      "[INFO] epoch: 4...\n",
      "epoch: 4 train loss: 0.021 train accuracy: 0.993\n",
      "[INFO] epoch: 5...\n",
      "epoch: 5 train loss: 0.021 train accuracy: 0.993\n",
      "[INFO] epoch: 6...\n",
      "epoch: 6 train loss: 0.021 train accuracy: 0.993\n",
      "[INFO] epoch: 7...\n",
      "epoch: 7 train loss: 0.020 train accuracy: 0.993\n",
      "[INFO] epoch: 8...\n",
      "epoch: 8 train loss: 0.020 train accuracy: 0.993\n",
      "[INFO] epoch: 9...\n",
      "epoch: 9 train loss: 0.020 train accuracy: 0.993\n",
      "[INFO] epoch: 10...\n",
      "epoch: 10 train loss: 0.020 train accuracy: 0.993\n",
      "CPU times: user 6h 53min 55s, sys: 11 s, total: 6h 54min 6s\n",
      "Wall time: 6min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_training_model(inFeatures=4, hiddenDim=8, nbClasses=3):\n",
    "    # construct a shallow, sequential neural network\n",
    "    mlpModel = nn.Sequential(OrderedDict([\n",
    "        (\"hidden_layer_1\", nn.Linear(inFeatures, hiddenDim)),\n",
    "        (\"activation_1\", nn.ReLU()),\n",
    "        (\"output_layer\", nn.Linear(hiddenDim, nbClasses))\n",
    "    ]))\n",
    "    # return the sequential model\n",
    "    return mlpModel\n",
    "\n",
    "def next_batch(inputs, targets, batchSize):\n",
    "    # loop over the dataset\n",
    "    for i in range(0, inputs.shape[0], batchSize):\n",
    "        # yield a tuple of the current batched data and labels\n",
    "        yield (inputs[i:i + batchSize], targets[i:i + batchSize])\n",
    "        \n",
    "# specify our batch size, number of epochs, and learning rate\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LR = 1e-2\n",
    "# determine the device we will be using for training\n",
    "DEVICE = \"cpu\"\n",
    "print(\"[INFO] training using {}...\".format(DEVICE))\n",
    "\n",
    "# generate a 3-class classification problem with 1000 data points,\n",
    "# where each data point is a 4D feature vector\n",
    "print(\"[INFO] preparing data...\")\n",
    "(X, y) = make_blobs(n_samples=10000000, n_features=4, centers=3,\n",
    "    cluster_std=2.5, random_state=95)\n",
    "# create training and testing splits, and convert them to PyTorch\n",
    "# tensors\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, y,\n",
    "    test_size=0.15, random_state=95)\n",
    "trainX = torch.from_numpy(trainX).float()\n",
    "testX = torch.from_numpy(testX).float()\n",
    "trainY = torch.from_numpy(trainY).float()\n",
    "testY = torch.from_numpy(testY).float()\n",
    "\n",
    "# initialize our model and display its architecture\n",
    "mlp = get_training_model().to(DEVICE)\n",
    "print(mlp)\n",
    "# initialize optimizer and loss function\n",
    "opt = SGD(mlp.parameters(), lr=LR)\n",
    "lossFunc = nn.CrossEntropyLoss()\n",
    "\n",
    "# create a template to summarize current training progress\n",
    "trainTemplate = \"epoch: {} test loss: {:.3f} test accuracy: {:.3f}\"\n",
    "# loop through the epochs\n",
    "for epoch in range(0, EPOCHS):\n",
    "    # initialize tracker variables and set our model to trainable\n",
    "    print(\"[INFO] epoch: {}...\".format(epoch + 1))\n",
    "    trainLoss = 0\n",
    "    trainAcc = 0\n",
    "    samples = 0\n",
    "    mlp.train()\n",
    "    # loop over the current batch of data\n",
    "    for (batchX, batchY) in next_batch(trainX, trainY, BATCH_SIZE):\n",
    "        # flash data to the current device, run it through our\n",
    "        # model, and calculate loss\n",
    "        (batchX, batchY) = (batchX.to(DEVICE), batchY.to(DEVICE))\n",
    "        predictions = mlp(batchX)\n",
    "        loss = lossFunc(predictions, batchY.long())\n",
    "        # zero the gradients accumulated from the previous steps,\n",
    "        # perform backpropagation, and update model parameters\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        # update training loss, accuracy, and the number of samples\n",
    "        # visited\n",
    "        trainLoss += loss.item() * batchY.size(0)\n",
    "        trainAcc += (predictions.max(1)[1] == batchY).sum().item()\n",
    "        samples += batchY.size(0)\n",
    "    # display model progress on the current training batch\n",
    "    trainTemplate = \"epoch: {} train loss: {:.3f} train accuracy: {:.3f}\"\n",
    "    print(trainTemplate.format(epoch + 1, (trainLoss / samples),\n",
    "        (trainAcc / samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5875b4b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058dcec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c2f4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biteme",
   "language": "python",
   "name": "biteme"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
